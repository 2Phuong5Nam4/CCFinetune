# **BÃ¡o CÃ¡o Ká»¹ Thuáº­t ChuyÃªn SÃ¢u: Kiáº¿n TrÃºc Agentic Fine-tuning vÃ  Triá»ƒn Khai Chatbot L1 Sá»­ Dá»¥ng Unsloth vÃ  GRPO**

## **TÃ³m táº¯t Äiá»u hÃ nh**

Sá»± chuyá»ƒn dá»‹ch cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o doanh nghiá»‡p vÃ o cuá»‘i nÄƒm 2025 Ä‘ang Ä‘Ã¡nh dáº¥u má»™t bÆ°á»›c ngoáº·t quan trá»ng tá»« cÃ¡c há»‡ thá»‘ng chatbot thá»¥ Ä‘á»™ng sang cÃ¡c mÃ´ hÃ¬nh tÃ¡c nhÃ¢n (agentic models) cÃ³ kháº£ nÄƒng tá»± chá»§ cao. YÃªu cáº§u Ä‘áº·t ra khÃ´ng cÃ²n dá»«ng láº¡i á»Ÿ viá»‡c truy xuáº¥t thÃ´ng tin (RAG), mÃ  lÃ  xÃ¢y dá»±ng cÃ¡c Chatbot Cáº¥p Ä‘á»™ 1 (L1) cÃ³ kháº£ nÄƒng tÆ° duy nhÆ° má»™t nhÃ¢n viÃªn chÃ­nh thá»©c: hiá»ƒu sÃ¢u sáº¯c thuáº­t ngá»¯ chuyÃªn ngÃ nh, láº­p káº¿ hoáº¡ch xá»­ lÃ½ quy trÃ¬nh (SOP), Ã¡nh xáº¡ káº¿ hoáº¡ch Ä‘Ã³ thÃ nh cÃ¡c lá»‡nh gá»i cÃ´ng cá»¥ (tool calls) chÃ­nh xÃ¡c, vÃ  giao tiáº¿p vá»›i vÄƒn phong mang Ä‘áº­m báº£n sáº¯c doanh nghiá»‡p.1

BÃ¡o cÃ¡o nÃ y cung cáº¥p má»™t phÃ¢n tÃ­ch ká»¹ thuáº­t toÃ n diá»‡n vá» quy trÃ¬nh "Agentic Fine-tuning" (Tinh chá»‰nh tÃ¡c nhÃ¢n), táº­p trung vÃ o viá»‡c sá»­ dá»¥ng thÆ° viá»‡n **Unsloth** Ä‘á»ƒ tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t pháº§n cá»©ng. Unsloth Ä‘Ã³ng vai trÃ² then chá»‘t nhá» kháº£ nÄƒng giáº£m má»©c tiÃªu thá»¥ VRAM tá»›i 80% vÃ  tÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n lÃªn gáº¥p 2 láº§n, cho phÃ©p triá»ƒn khai cÃ¡c ká»¹ thuáº­t tiÃªn tiáº¿n nhÆ° **Group Relative Policy Optimization (GRPO)** ngay cáº£ trÃªn háº¡ táº§ng GPU háº¡n cháº¿.3 BÃ¡o cÃ¡o sáº½ Ä‘i sÃ¢u vÃ o ba trá»¥ cá»™t chÃ­nh: (1) XÃ¢y dá»±ng "NhÃ  mÃ¡y dá»¯ liá»‡u" (Data Factory) Ä‘á»ƒ chuyá»ƒn Ä‘á»•i tÃ i liá»‡u tÄ©nh thÃ nh chuá»—i tÆ° duy Ä‘á»™ng, (2) Thiáº¿t láº­p pipeline huáº¥n luyá»‡n káº¿t há»£p SFT vÃ  GRPO Ä‘á»ƒ Ä‘á»‹nh hÃ¬nh tÆ° duy vÃ  vÄƒn phong, vÃ  (3) Chiáº¿n lÆ°á»£c triá»ƒn khai thá»±c táº¿ trÃªn Unsloth.

## ---

**1\. CÆ¡ Sá»Ÿ LÃ½ Luáº­n vÃ  Kiáº¿n TrÃºc Cá»§a Chatbot L1**

### **1.1 Sá»± Chuyá»ƒn Dá»‹ch Tá»« Truy Xuáº¥t (Retrieval) Sang Thá»±c Thi (Execution)**

Trong cÃ¡c há»‡ thá»‘ng AI doanh nghiá»‡p truyá»n thá»‘ng, mÃ´ hÃ¬nh RAG (Retrieval-Augmented Generation) Ä‘Ã³ng vai trÃ² chá»§ Ä‘áº¡o trong viá»‡c cung cáº¥p kiáº¿n thá»©c. Tuy nhiÃªn, RAG bá»™c lá»™ nhá»¯ng háº¡n cháº¿ nghiÃªm trá»ng khi Ä‘á»‘i máº·t vá»›i yÃªu cáº§u vá» *quy trÃ¬nh* vÃ  *hÃ nh Ä‘á»™ng*. Má»™t mÃ´ hÃ¬nh RAG cÃ³ thá»ƒ tÃ¬m tháº¥y tÃ i liá»‡u quy Ä‘á»‹nh vá» "Quy trÃ¬nh hoÃ n tiá»n", nhÆ°ng thÆ°á»ng tháº¥t báº¡i trong viá»‡c tá»± Ä‘á»™ng phÃ¢n rÃ£ quy trÃ¬nh Ä‘Ã³ thÃ nh cÃ¡c bÆ°á»›c thá»±c thi tuáº§n tá»± hoáº·c tháº¥t báº¡i trong viá»‡c duy trÃ¬ giá»ng Ä‘iá»‡u chuyÃªn nghiá»‡p cá»§a má»™t nhÃ¢n viÃªn chÄƒm sÃ³c khÃ¡ch hÃ ng xuyÃªn suá»‘t cuá»™c há»™i thoáº¡i.4

Chatbot L1 Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi kháº£ nÄƒng "Suy nghÄ©, Láº­p káº¿ hoáº¡ch vÃ  HÃ nh Ä‘á»™ng" (Think, Plan, Act). QuÃ¡ trÃ¬nh tinh chá»‰nh (fine-tuning) khÃ´ng chá»‰ Ä‘Æ¡n thuáº§n lÃ  náº¡p kiáº¿n thá»©c, mÃ  lÃ  thay Ä‘á»•i trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ hÃ¬nh thÃ nh má»™t *trá»±c giÃ¡c quy trÃ¬nh*. Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh:

1. **Hiá»ƒu thuáº­t ngá»¯ ná»™i bá»™:** Náº¯m báº¯t cÃ¡c tá»« viáº¿t táº¯t, mÃ£ dá»± Ã¡n vÃ  biá»‡t ngá»¯ chuyÃªn ngÃ nh mÃ  khÃ´ng cáº§n giáº£i thÃ­ch ngá»¯ cáº£nh liÃªn tá»¥c.6  
2. **Láº­p káº¿ hoáº¡ch suy luáº­n (Chain-of-Thought \- CoT):** Tá»± Ä‘á»™ng sinh ra cÃ¡c chuá»—i suy nghÄ© ná»™i bá»™ Ä‘á»ƒ phÃ¢n tÃ­ch váº¥n Ä‘á» trÆ°á»›c khi Ä‘Æ°a ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng, tÆ°Æ¡ng tá»± nhÆ° cÃ¡ch con ngÆ°á»i tÆ° duy.7  
3. **Äá»“ng bá»™ hÃ³a vÄƒn phong (Persona Alignment):** Pháº£n há»“i vá»›i giá»ng Ä‘iá»‡u, cáº¥u trÃºc cÃ¢u vÃ  thÃ¡i Ä‘á»™ phÃ¹ há»£p vá»›i vÄƒn hÃ³a cÃ´ng ty, Ä‘iá»u mÃ  prompt engineering khÃ³ duy trÃ¬ á»•n Ä‘á»‹nh.9

### **1.2 Vai TrÃ² Cá»§a Unsloth Trong Ká»· NguyÃªn Agentic AI**

Viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng suy luáº­n vÃ  gá»i cÃ´ng cá»¥ Ä‘Ã²i há»i tÃ i nguyÃªn tÃ­nh toÃ¡n khá»•ng lá»“, Ä‘áº·c biá»‡t lÃ  khi Ã¡p dá»¥ng Reinforcement Learning (RL). Unsloth ná»•i lÃªn nhÆ° má»™t giáº£i phÃ¡p háº¡ táº§ng thiáº¿t yáº¿u nhá» vÃ o viá»‡c viáº¿t láº¡i cÃ¡c háº¡t nhÃ¢n (kernels) tÃ­nh toÃ¡n cá»§a PyTorch báº±ng ngÃ´n ngá»¯ Triton vÃ  thá»±c hiá»‡n Ä‘áº¡o hÃ m thá»§ cÃ´ng (manual autograd).10

PhÃ¢n tÃ­ch ká»¹ thuáº­t cho tháº¥y Unsloth mang láº¡i hai lá»£i tháº¿ chiáº¿n lÆ°á»£c cho viá»‡c xÃ¢y dá»±ng Chatbot L1:

* **Hiá»‡u quáº£ bá»™ nhá»› cho ngá»¯ cáº£nh dÃ i:** Äá»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ "suy nghÄ©" (táº¡o ra cÃ¡c token \<think\>), ngá»¯ cáº£nh huáº¥n luyá»‡n thÆ°á»ng pháº£i ráº¥t dÃ i (tá»« 8k Ä‘áº¿n 128k token). Unsloth giáº£m má»©c sá»­ dá»¥ng VRAM xuá»‘ng 60-80% thÃ´ng qua viá»‡c tá»‘i Æ°u hÃ³a Flash Attention vÃ  quáº£n lÃ½ bá»™ nhá»› Ä‘á»‡m KV (Key-Value), cho phÃ©p huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh Llama-3 hoáº·c Mistral vá»›i ngá»¯ cáº£nh dÃ i trÃªn má»™t GPU Ä‘Æ¡n láº».3  
* **Há»— trá»£ GRPO native:** Unsloth cung cáº¥p cÃ¡c báº£n cÃ i Ä‘áº·t tá»‘i Æ°u cho thuáº­t toÃ¡n GRPO, loáº¡i bá» nhu cáº§u vá» má»™t mÃ´ hÃ¬nh "Critic" (PhÃª bÃ¬nh) riÃªng biá»‡t vá»‘n tiÃªu tá»‘n nhiá»u bá»™ nhá»› trong cÃ¡c phÆ°Æ¡ng phÃ¡p PPO truyá»n thá»‘ng, tá»« Ä‘Ã³ dÃ¢n chá»§ hÃ³a viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh suy luáº­n.12

## ---

**2\. Chiáº¿n LÆ°á»£c Dá»¯ Liá»‡u: XÃ¢y Dá»±ng "Data Factory" Cho Agent**

Cháº¥t lÆ°á»£ng cá»§a má»™t agent Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh bá»Ÿi cháº¥t lÆ°á»£ng cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n. Äá»‘i vá»›i má»¥c tiÃªu xÃ¢y dá»±ng má»™t nhÃ¢n viÃªn áº£o, dá»¯ liá»‡u thÃ´ tá»« cÃ¡c file PDF hay tÃ i liá»‡u hÆ°á»›ng dáº«n váº­n hÃ nh (SOP) lÃ  khÃ´ng Ä‘á»§. ChÃºng ta cáº§n thiáº¿t láº­p má»™t quy trÃ¬nh chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u, hay cÃ²n gá»i lÃ  "Data Factory", Ä‘á»ƒ biáº¿n cÃ¡c vÄƒn báº£n tÄ©nh thÃ nh cÃ¡c máº«u huáº¥n luyá»‡n chá»©a Ä‘á»±ng *logic suy luáº­n* vÃ  *cáº¥u trÃºc hÃ nh Ä‘á»™ng*.

### **2.1 Cáº¥u TrÃºc Dá»¯ Liá»‡u Má»¥c TiÃªu (Target Data Topology)**

Dá»¯ liá»‡u Ä‘áº§u vÃ o cho quÃ¡ trÃ¬nh fine-tuning trÃªn Unsloth cáº§n tuÃ¢n thá»§ Ä‘á»‹nh dáº¡ng JSONL, nhÆ°ng ná»™i dung bÃªn trong pháº£i chá»©a Ä‘á»±ng ba thÃ nh pháº§n cá»‘t lÃµi Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu Ä‘á» ra: TÆ° duy (Reasoning), Gá»i cÃ´ng cá»¥ (Tool Call), vÃ  VÄƒn phong (Style).

Báº£ng dÆ°á»›i Ä‘Ã¢y mÃ´ táº£ cáº¥u trÃºc cá»§a má»™t máº«u dá»¯ liá»‡u lÃ½ tÆ°á»Ÿng cho Chatbot L1:

| ThÃ nh pháº§n | Äá»‹nh dáº¡ng ká»¹ thuáº­t | Má»¥c Ä‘Ã­ch huáº¥n luyá»‡n |
| :---- | :---- | :---- |
| **System Prompt** | Chá»©a Ä‘á»‹nh nghÄ©a Tools (JSON Schema) vÃ  chá»‰ thá»‹ vá» Persona. | Thiáº¿t láº­p khÃ´ng gian hÃ nh Ä‘á»™ng vÃ  vai trÃ² cá»§a agent.14 |
| **User Query** | CÃ¢u há»i hoáº·c yÃªu cáº§u mÃ´ phá»ng tá»« ngÆ°á»i dÃ¹ng thá»±c táº¿. | Táº¡o ngá»¯ cáº£nh Ä‘áº§u vÃ o Ä‘a dáº¡ng (Ä‘á»§ thÃ´ng tin, thiáº¿u thÃ´ng tin, mÆ¡ há»“).16 |
| **Reasoning Trace** | Äáº·t trong tháº» \<think\>...\</think\>. | Dáº¡y mÃ´ hÃ¬nh cÃ¡ch phÃ¢n tÃ­ch SOP, kiá»ƒm tra Ä‘iá»u kiá»‡n vÃ  láº­p káº¿ hoáº¡ch trÆ°á»›c khi tráº£ lá»i.8 |
| **Tool Call** | Äá»‹nh dáº¡ng JSON hoáº·c Python function call. | Dáº¡y mÃ´ hÃ¬nh Ã¡nh xáº¡ káº¿ hoáº¡ch thÃ nh hÃ nh Ä‘á»™ng mÃ¡y tÃ­nh chÃ­nh xÃ¡c.17 |
| **Final Response** | VÄƒn báº£n tá»± nhiÃªn bao quanh tool call. | Huáº¥n luyá»‡n vÄƒn phong, giá»ng Ä‘iá»‡u vÃ  cÃ¡ch giao tiáº¿p cá»§a nhÃ¢n viÃªn.9 |

### **2.2 Pipeline Tá»•ng Thá»ƒ: Tá»« SOP Äáº¿n Dataset**

Quy trÃ¬nh xÃ¢y dá»±ng dá»¯ liá»‡u khÃ´ng nÃªn thá»±c hiá»‡n thá»§ cÃ´ng mÃ  cáº§n Ä‘Æ°á»£c tá»± Ä‘á»™ng hÃ³a thÃ´ng qua má»™t pipeline sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n hÆ¡n (Teacher Models) Ä‘á»ƒ sinh dá»¯ liá»‡u cho mÃ´ hÃ¬nh nhá» hÆ¡n (Student Models).

#### **Giai Ä‘oáº¡n 1: PhÃ¢n rÃ£ vÃ  Chunking TÃ i liá»‡u (Ingestion)**

BÆ°á»›c Ä‘áº§u tiÃªn lÃ  xá»­ lÃ½ cÃ¡c tÃ i liá»‡u quy trÃ¬nh (SOP, Policy PDF). Viá»‡c cáº¯t nhá» vÄƒn báº£n (chunking) khÃ´ng thá»ƒ thá»±c hiá»‡n ngáº«u nhiÃªn theo sá»‘ lÆ°á»£ng token mÃ  pháº£i dá»±a trÃªn *ngá»¯ nghÄ©a quy trÃ¬nh*.

* **Ká»¹ thuáº­t:** Sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n nhÆ° PyMuPDF hoáº·c Unstructured Ä‘á»ƒ trÃ­ch xuáº¥t vÄƒn báº£n nhÆ°ng giá»¯ láº¡i cáº¥u trÃºc tiÃªu Ä‘á». Má»—i "chunk" nÃªn tÆ°Æ¡ng á»©ng vá»›i má»™t quy trÃ¬nh nghiá»‡p vá»¥ hoÃ n chá»‰nh (vÃ­ dá»¥: "Quy trÃ¬nh xá»­ lÃ½ khiáº¿u náº¡i Ä‘á»•i tráº£").18  
* **LÃ½ do:** Náº¿u cáº¯t Ä‘Ã´i má»™t quy trÃ¬nh, mÃ´ hÃ¬nh Teacher sáº½ khÃ´ng cÃ³ Ä‘á»§ ngá»¯ cáº£nh Ä‘á»ƒ sinh ra má»™t chuá»—i suy luáº­n logic Ä‘áº§y Ä‘á»§, dáº«n Ä‘áº¿n dá»¯ liá»‡u huáº¥n luyá»‡n bá»‹ gÃ£y vá»¥n (hallucinated reasoning).20

#### **Giai Ä‘oáº¡n 2: Tá»•ng há»£p Dá»¯ liá»‡u Suy luáº­n (Synthetic Reasoning Generation)**

ÄÃ¢y lÃ  trÃ¡i tim cá»§a Data Factory. ChÃºng ta sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh máº¡nh (nhÆ° GPT-4o, Claude 3.5 Sonnet hoáº·c DeepSeek-V3) Ä‘á»ƒ Ä‘Ã³ng vai trÃ² "GiÃ¡o viÃªn", chuyá»ƒn Ä‘á»•i vÄƒn báº£n SOP thÃ nh cÃ¡c Ä‘oáº¡n há»™i thoáº¡i cÃ³ kÃ¨m suy luáº­n.

Ká»¹ thuáº­t Prompting cho Teacher Model:  
Prompt gá»­i cho Teacher Model cáº§n Ä‘Æ°á»£c thiáº¿t káº¿ ká»¹ lÆ°á»¡ng Ä‘á»ƒ Ã©p mÃ´ hÃ¬nh nÃ y "suy nghÄ© ra ngoÃ i" (externalize thoughts).

* *YÃªu cáº§u 1:* "ÄÃ³ng vai má»™t chuyÃªn gia Ä‘Ã o táº¡o nhÃ¢n sá»±. Äá»c quy trÃ¬nh SOP dÆ°á»›i Ä‘Ã¢y vÃ  táº¡o ra má»™t tÃ¬nh huá»‘ng khÃ¡ch hÃ ng thá»±c táº¿."  
* *YÃªu cáº§u 2 (Táº¡o suy luáº­n):* "HÃ£y viáº¿t ra má»™t chuá»—i suy nghÄ© ná»™i tÃ¢m (\<think\>) nÆ¡i nhÃ¢n viÃªn phÃ¢n tÃ­ch yÃªu cáº§u cá»§a khÃ¡ch, Ä‘á»‘i chiáº¿u vá»›i cÃ¡c Ä‘iá»u kiá»‡n trong SOP, xÃ¡c Ä‘á»‹nh thÃ´ng tin cÃ²n thiáº¿u, vÃ  quyáº¿t Ä‘á»‹nh bÆ°á»›c tiáº¿p theo.".16  
* *YÃªu cáº§u 3 (Ãnh xáº¡ cÃ´ng cá»¥):* "Dá»±a trÃªn káº¿ hoáº¡ch, hÃ£y táº¡o ra lá»‡nh gá»i cÃ´ng cá»¥ (JSON) chÃ­nh xÃ¡c theo schema sau..."  
* *YÃªu cáº§u 4 (VÄƒn phong):* "Viáº¿t cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng vá»›i giá»ng Ä‘iá»‡u chuyÃªn nghiá»‡p, Ä‘á»“ng cáº£m nhÆ°ng quyáº¿t Ä‘oÃ¡n, sá»­ dá»¥ng Ä‘Ãºng thuáº­t ngá»¯ ná»™i bá»™ nhÆ° 'Ticket', 'Escalation'...".22

#### **Giai Ä‘oáº¡n 3: Kiá»ƒm Ä‘á»‹nh vÃ  Äá»‹nh dáº¡ng (Validation & Formatting)**

Dá»¯ liá»‡u sinh ra cáº§n Ä‘Æ°á»£c kiá»ƒm tra tá»± Ä‘á»™ng Ä‘á»ƒ loáº¡i bá» cÃ¡c máº«u kÃ©m cháº¥t lÆ°á»£ng.

* **Validation Script:** Sá»­ dá»¥ng Python script Ä‘á»ƒ parse cÃ¡c lá»‡nh JSON trong tool call. Náº¿u JSON khÃ´ng Ä‘Ãºng cÃº phÃ¡p hoáº·c tham sá»‘ khÃ´ng khá»›p vá»›i schema, máº«u dá»¯ liá»‡u Ä‘Ã³ sáº½ bá»‹ loáº¡i bá» hoáº·c Ä‘Æ°a vÃ o quy trÃ¬nh sá»­a lá»—i tá»± Ä‘á»™ng (Self-Correction Loop).14  
* **Unsloth Formatting:** Cuá»‘i cÃ¹ng, dá»¯ liá»‡u sáº¡ch Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i sang Ä‘á»‹nh dáº¡ng Chat Template mÃ  Unsloth há»— trá»£ (thÆ°á»ng lÃ  Ä‘á»‹nh dáº¡ng Alpaca hoáº·c ShareGPT), Ä‘áº£m báº£o cÃ¡c tháº» Ä‘áº·c biá»‡t (special tokens) Ä‘Æ°á»£c xá»­ lÃ½ Ä‘Ãºng.18

## ---

**3\. Pipeline Fine-tuning TrÃªn Unsloth: Triá»ƒn Khai Ká»¹ Thuáº­t**

Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu táº¡o ra má»™t agent vá»«a hiá»ƒu biáº¿t sÃ¢u (knowledgeable), vá»«a tÆ° duy tá»‘t (reasoning), vá»«a hÃ nh Ä‘á»™ng chuáº©n (actionable), vÃ  cÃ³ vÄƒn phong thá»±c (stylistic), chÃºng tÃ´i Ä‘á» xuáº¥t má»™t chiáº¿n lÆ°á»£c huáº¥n luyá»‡n hai giai Ä‘oáº¡n: **Supervised Fine-Tuning (SFT)** Ä‘á»ƒ náº¡p kiáº¿n thá»©c vÃ  **Group Relative Policy Optimization (GRPO)** Ä‘á»ƒ rÃ¨n luyá»‡n tÆ° duy vÃ  vÄƒn phong.

### **3.1 Thiáº¿t Láº­p MÃ´i TrÆ°á»ng Unsloth**

Viá»‡c thiáº¿t láº­p mÃ´i trÆ°á»ng Unsloth Ä‘Ã²i há»i sá»± tÆ°Æ¡ng thÃ­ch cháº·t cháº½ giá»¯a cÃ¡c thÆ° viá»‡n Ä‘á»ƒ táº­n dá»¥ng tá»‘i Ä‘a kháº£ nÄƒng tÄƒng tá»‘c pháº§n cá»©ng. Unsloth yÃªu cáº§u GPU NVIDIA (há»— trá»£ tá»‘t nháº¥t tá»« dÃ²ng Ampere trá»Ÿ lÃªn nhÆ° A100, H100, hoáº·c dÃ²ng tiÃªu dÃ¹ng RTX 3090/4090) vÃ  há»‡ Ä‘iá»u hÃ nh Linux.3

MÃ£ cÃ i Ä‘áº·t cÆ¡ báº£n trong mÃ´i trÆ°á»ng áº£o:

Bash

conda create \--name unsloth\_env python=3.10  
conda activate unsloth\_env  
pip install unsloth vllm  
pip install \--no-deps "xformers\<0.0.27" "trl\<0.9.0" peft accelerate bitsandbytes

*LÆ°u Ã½:* Viá»‡c cÃ i Ä‘áº·t vllm cÃ¹ng vá»›i unsloth lÃ  báº¯t buá»™c cho giai Ä‘oáº¡n GRPO Ä‘á»ƒ tÄƒng tá»‘c quÃ¡ trÃ¬nh sinh dá»¯ liá»‡u (rollout generation).25

### **3.2 Giai Ä‘oáº¡n 1: Supervised Fine-Tuning (SFT) \- Náº¡p Kiáº¿n Thá»©c vÃ  Cáº¥u TrÃºc**

Má»¥c tiÃªu cá»§a giai Ä‘oáº¡n nÃ y lÃ  dáº¡y cho mÃ´ hÃ¬nh "biáº¿t" quy trÃ¬nh SOP, "biáº¿t" cÃ¡ch gá»i tool, vÃ  "biáº¿t" cáº¥u trÃºc \<think\>.

#### **Khá»Ÿi táº¡o MÃ´ hÃ¬nh**

Sá»­ dá»¥ng FastLanguageModel Ä‘á»ƒ táº£i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ (vÃ­ dá»¥: Llama-3.1-8B-Instruct) á»Ÿ cháº¿ Ä‘á»™ 4-bit quantization. Cháº¿ Ä‘á»™ nÃ y giáº£m táº£i bá»™ nhá»› VRAM xuá»‘ng 4 láº§n mÃ  gáº§n nhÆ° khÃ´ng lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c, cho phÃ©p huáº¥n luyá»‡n cÃ¡c batch size lá»›n hÆ¡n.13

Python

from unsloth import FastLanguageModel  
import torch

model, tokenizer \= FastLanguageModel.from\_pretrained(  
    model\_name \= "unsloth/Llama-3.1-8B-Instruct",  
    max\_seq\_length \= 8192, \# Cáº§n context dÃ i cho chuá»—i suy luáº­n  
    dtype \= None, \# Tá»± Ä‘á»™ng phÃ¡t hiá»‡n (thÆ°á»ng lÃ  bfloat16)  
    load\_in\_4bit \= True,  
)

#### **Cáº¥u hÃ¬nh LoRA (Low-Rank Adaptation)**

Äá»ƒ mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng suy luáº­n phá»©c táº¡p, viá»‡c Ã¡p dá»¥ng LoRA lÃªn táº¥t cáº£ cÃ¡c module tuyáº¿n tÃ­nh (linear modules) lÃ  ráº¥t quan trá»ng, thay vÃ¬ chá»‰ Ã¡p dá»¥ng lÃªn cÃ¡c lá»›p Attention nhÆ° truyá»n thá»‘ng.

Python

model \= FastLanguageModel.get\_peft\_model(  
    model,  
    r \= 64, \# Rank cao hÆ¡n (64-128) giÃºp há»c logic phá»©c táº¡p tá»‘t hÆ¡n  
    target\_modules \= \["q\_proj", "k\_proj", "v\_proj", "o\_proj",  
                      "gate\_proj", "up\_proj", "down\_proj"\],  
    lora\_alpha \= 16,  
    lora\_dropout \= 0,  
    bias \= "none",  
    use\_gradient\_checkpointing \= "unsloth",   
    random\_state \= 3407,  
)

.3

#### **Cáº¥u hÃ¬nh Trainer vÃ  Chat Template**

Äiá»ƒm quan trá»ng nháº¥t á»Ÿ Ä‘Ã¢y lÃ  sá»­ dá»¥ng DataCollatorForCompletionOnlyLM. Ká»¹ thuáº­t nÃ y Ä‘áº£m báº£o ráº±ng loss (hÃ m máº¥t mÃ¡t) chá»‰ Ä‘Æ°á»£c tÃ­nh trÃªn pháº§n pháº£n há»“i cá»§a agent (bao gá»“m suy nghÄ© vÃ  tool call), chá»© khÃ´ng tÃ­nh trÃªn pháº§n system prompt hay cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng. Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh táº­p trung tá»‘i Ä‘a vÃ o viá»‡c há»c cÃ¡ch *xá»­ lÃ½* váº¥n Ä‘á».28

### **3.3 Giai Ä‘oáº¡n 2: Group Relative Policy Optimization (GRPO) \- RÃ¨n Luyá»‡n TÆ° Duy vÃ  VÄƒn Phong**

Sau SFT, mÃ´ hÃ¬nh Ä‘Ã£ biáº¿t cÃ¡ch gá»i tool nhÆ°ng cÃ³ thá»ƒ váº«n cÃ²n "áº£o giÃ¡c" (hallucination) trong suy luáº­n hoáº·c vÄƒn phong chÆ°a tháº­t sá»± tá»± nhiÃªn. GRPO lÃ  bÆ°á»›c Ä‘á»™t phÃ¡ Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y.

#### **CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng cá»§a GRPO**

KhÃ¡c vá»›i PPO cáº§n má»™t mÃ´ hÃ¬nh Critic (tá»‘n gáº¥p Ä‘Ã´i VRAM), GRPO hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch sinh ra má»™t nhÃ³m (group) cÃ¡c cÃ¢u tráº£ lá»i (vÃ­ dá»¥: 8 cÃ¢u tráº£ lá»i) cho cÃ¹ng má»™t cÃ¢u há»i. Sau Ä‘Ã³, nÃ³ cháº¥m Ä‘iá»ƒm cÃ¡c cÃ¢u tráº£ lá»i nÃ y dá»±a trÃªn má»™t táº­p há»£p cÃ¡c hÃ m pháº§n thÆ°á»Ÿng (reward functions) vÃ  cáº­p nháº­t trá»ng sá»‘ Ä‘á»ƒ Æ°u tiÃªn cÃ¡c cÃ¢u tráº£ lá»i cÃ³ Ä‘iá»ƒm cao hÆ¡n trung bÃ¬nh cá»§a nhÃ³m.29

#### **Thiáº¿t káº¿ HÃ m Pháº§n ThÆ°á»Ÿng (Reward Functions)**

ÄÃ¢y lÃ  nÆ¡i chÃºng ta láº­p trÃ¬nh hÃ³a cÃ¡c yÃªu cáº§u vá» "vÄƒn phong giá»‘ng ngÆ°á»i tháº­t" vÃ  "quy trÃ¬nh chuáº©n". ChÃºng ta cáº§n xÃ¢y dá»±ng 3 loáº¡i hÃ m pháº§n thÆ°á»Ÿng:

1. **Pháº§n thÆ°á»Ÿng CÃº phÃ¡p (Format Reward):** Kiá»ƒm tra xem mÃ´ hÃ¬nh cÃ³ tuÃ¢n thá»§ cáº¥u trÃºc \<think\>...\</think\> vÃ  Ä‘á»‹nh dáº¡ng JSON cá»§a tool call hay khÃ´ng. ÄÃ¢y lÃ  Ä‘iá»u kiá»‡n tiÃªn quyáº¿t (Hard constraint).31  
   * *CÃ i Ä‘áº·t:* Sá»­ dá»¥ng Regex Ä‘á»ƒ parse output. Tráº£ vá» Ä‘iá»ƒm 1.0 náº¿u Ä‘Ãºng cÃº phÃ¡p, 0.0 náº¿u sai.  
2. **Pháº§n thÆ°á»Ÿng ChÃ­nh xÃ¡c (Correctness Reward):** Kiá»ƒm tra xem tool call Ä‘Æ°á»£c gá»i cÃ³ Ä‘Ãºng vá»›i quy trÃ¬nh SOP khÃ´ng.  
   * *CÃ i Ä‘áº·t:* So sÃ¡nh tÃªn hÃ m vÃ  tham sá»‘ trong tool call do mÃ´ hÃ¬nh sinh ra vá»›i tool call "Ground Truth" trong táº­p dá»¯ liá»‡u tá»•ng há»£p.  
3. **Pháº§n thÆ°á»Ÿng VÄƒn phong (Style/Persona Reward):** ÄÃ¢y lÃ  yáº¿u tá»‘ giÃºp agent "giá»‘ng ngÆ°á»i tháº­t".  
   * *CÃ i Ä‘áº·t:* ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh embedding nhá» hoáº·c cÃ¡c Ä‘á»™ Ä‘o ngÃ´n ngá»¯ Ä‘á»ƒ so sÃ¡nh vÄƒn báº£n pháº£n há»“i cá»§a agent vá»›i cÃ¡c máº«u vÄƒn báº£n chuáº©n cá»§a nhÃ¢n viÃªn xuáº¥t sáº¯c. Náº¿u Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cao, mÃ´ hÃ¬nh nháº­n Ä‘iá»ƒm thÆ°á»Ÿng. Hoáº·c Ä‘Æ¡n giáº£n hÆ¡n, pháº¡t Ä‘iá»ƒm náº¿u mÃ´ hÃ¬nh sá»­ dá»¥ng cÃ¡c cá»¥m tá»« "robot" nhÆ° "I am an AI model".22

#### **Triá»ƒn khai GRPO Trainer trÃªn Unsloth**

Unsloth há»— trá»£ trá»±c tiáº¿p GRPOTrainer tá»« thÆ° viá»‡n TRL nhÆ°ng Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a háº¡t nhÃ¢n.

Python

from trl import GRPOConfig, GRPOTrainer

training\_args \= GRPOConfig(  
    output\_dir \= "grpo\_agent\_output",  
    learning\_rate \= 1e-6, \# Learning rate ráº¥t tháº¥p cho RL  
    num\_generations \= 8,  \# KÃ­ch thÆ°á»›c nhÃ³m (Group Size)  
    max\_completion\_length \= 1024, \# DÃ nh khÃ´ng gian cho suy luáº­n  
    beta \= 0.1, \# Há»‡ sá»‘ pháº¡t KL Divergence  
    use\_vllm \= True, \# TÃ­ch há»£p vLLM Ä‘á»ƒ tÄƒng tá»‘c sinh dá»¯ liá»‡u  
    vllm\_gpu\_memory\_utilization \= 0.5, \# Chia sáº» VRAM  
)

trainer \= GRPOTrainer(  
    model \= model,  
    reward\_funcs \= \[format\_reward\_func, tool\_accuracy\_func, style\_reward\_func\],  
    args \= training\_args,  
    train\_dataset \= dataset,  
)  
trainer.train()

.25

Sá»± káº¿t há»£p giá»¯a use\_vllm=True vÃ  cÆ¡ cháº¿ quáº£n lÃ½ bá»™ nhá»› cá»§a Unsloth cho phÃ©p quÃ¡ trÃ¬nh sinh dá»¯ liá»‡u (rollout) diá»…n ra cá»±c nhanh ngay trÃªn cÃ¹ng má»™t GPU Ä‘ang huáº¥n luyá»‡n, Ä‘iá»u mÃ  cÃ¡c thÆ° viá»‡n khÃ¡c thÆ°á»ng gáº·p khÃ³ khÄƒn do ngháº½n cá»• chai bá»™ nhá»›.3

## ---

**4\. Tá»•ng Quan Vá» Dataset vÃ  Chiáº¿n LÆ°á»£c XÃ¢y Dá»±ng**

Äá»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ "dÆ°a ra planning xá»­ lÃ½ nhÆ° má»™t nhÃ¢n viÃªn chÃ­nh thá»©c", táº­p dá»¯ liá»‡u khÃ´ng thá»ƒ chá»‰ lÃ  cÃ¡c cáº·p Há»i-ÄÃ¡p Ä‘Æ¡n giáº£n. NÃ³ pháº£i lÃ  táº­p há»£p cá»§a cÃ¡c "ká»‹ch báº£n xá»­ lÃ½" (processing scenarios).

### **4.1 Chi Tiáº¿t Vá» Record Dá»¯ Liá»‡u**

Má»—i báº£n ghi trong dataset (dÃ²ng trong file JSONL) Ä‘áº¡i diá»‡n cho má»™t phiÃªn lÃ m viá»‡c xá»­ lÃ½ sá»± cá»‘.

JSON

{  
  "messages":  
}

*LÆ°u Ã½:* Trong vÃ­ dá»¥ trÃªn, pháº§n \<think\> thá»ƒ hiá»‡n rÃµ logic cá»§a má»™t nhÃ¢n viÃªn Ä‘ang tuÃ¢n thá»§ quy trÃ¬nh. Pháº§n \<tool\_call\> lÃ  hÃ nh Ä‘á»™ng cá»¥ thá»ƒ. Náº¿u khÃ´ng cÃ³ pháº§n \<think\>, mÃ´ hÃ¬nh cÃ³ thá»ƒ sáº½ "Ä‘oÃ¡n mÃ²" vÃ  thá»±c hiá»‡n hoÃ n tiá»n ngay láº­p tá»©c, vi pháº¡m quy cháº¿ cÃ´ng ty.15

### **4.2 Tá»± Äá»™ng HÃ³a Viá»‡c XÃ¢y Dá»±ng Dataset**

Äá»ƒ táº¡o ra hÃ ng nghÃ¬n máº«u dá»¯ liá»‡u nhÆ° trÃªn tá»« hÃ ng trÄƒm trang tÃ i liá»‡u PDF, chÃºng ta cáº§n má»™t pipeline tá»± Ä‘á»™ng hÃ³a:

1. **TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng (Feature Extraction):** PhÃ¢n tÃ­ch PDF Ä‘á»ƒ tÃ¡ch riÃªng pháº§n "Äiá»u kiá»‡n" (Conditions) vÃ  "HÃ nh Ä‘á»™ng" (Actions). VÃ­ dá»¥: "Náº¿u \[Äiá»u kiá»‡n A\] thÃ¬ thá»±c hiá»‡n".  
2. **Ká»‹ch báº£n hÃ³a (Scenario Generation):** Sá»­ dá»¥ng LLM Teacher Ä‘á»ƒ táº¡o ra cÃ¡c ká»‹ch báº£n ngÆ°á»i dÃ¹ng rÆ¡i vÃ o cÃ¡c nhÃ¡nh Ä‘iá»u kiá»‡n khÃ¡c nhau (vÃ­ dá»¥: Ká»‹ch báº£n ngÆ°á»i dÃ¹ng thá»a mÃ£n Ä‘iá»u kiá»‡n A, vÃ  ká»‹ch báº£n ngÆ°á»i dÃ¹ng vi pháº¡m Ä‘iá»u kiá»‡n A).  
3. **LÃ m giÃ u vÄƒn phong (Style Injection):** YÃªu cáº§u LLM Teacher viáº¿t láº¡i cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng dá»±a trÃªn bá»™ "Style Guide" cá»§a cÃ´ng ty (vÃ­ dá»¥: "LuÃ´n báº¯t Ä‘áº§u báº±ng lá»i xin lá»—i náº¿u tá»« chá»‘i", "Sá»­ dá»¥ng ngÃ´n ngá»¯ tÃ­ch cá»±c").  
4. **Kiá»ƒm tra tÃ­nh nháº¥t quÃ¡n (Consistency Check):** Má»™t script tá»± Ä‘á»™ng sáº½ cháº¡y láº¡i logic cá»§a máº«u dá»¯ liá»‡u. Náº¿u pháº§n suy luáº­n \<think\> dáº«n Ä‘áº¿n káº¿t luáº­n A, nhÆ°ng tool call láº¡i thá»±c hiá»‡n hÃ nh Ä‘á»™ng B, máº«u dá»¯ liá»‡u Ä‘Ã³ sáº½ bá»‹ loáº¡i bá».16

## ---

**5\. Triá»ƒn Khai Thá»±c Táº¿ vÃ  TÆ°Æ¡ng Lai**

### **5.1 Kiáº¿n TrÃºc Suy Luáº­n (Inference Architecture)**

Sau khi tinh chá»‰nh thÃ nh cÃ´ng, mÃ´ hÃ¬nh Ä‘Æ°á»£c há»£p nháº¥t (merge) cÃ¡c trá»ng sá»‘ LoRA vÃ o mÃ´ hÃ¬nh gá»‘c. QuÃ¡ trÃ¬nh triá»ƒn khai thá»±c táº¿ (Production) cáº§n má»™t kiáº¿n trÃºc suy luáº­n Ä‘áº·c biá»‡t Ä‘á»ƒ xá»­ lÃ½ chuá»—i tÆ° duy:

* **áº¨n luá»“ng tÆ° duy:** Trong giao diá»‡n ngÆ°á»i dÃ¹ng (UI Chatbot), pháº§n ná»™i dung trong tháº» \<think\>...\</think\> cáº§n Ä‘Æ°á»£c áº©n Ä‘i. NgÆ°á»i dÃ¹ng cuá»‘i chá»‰ nhÃ¬n tháº¥y cÃ¢u tráº£ lá»i tá»± nhiÃªn hoáº·c káº¿t quáº£ thá»±c thi cÃ´ng cá»¥. Tuy nhiÃªn, luá»“ng tÆ° duy nÃ y cáº§n Ä‘Æ°á»£c lÆ°u log Ä‘á»ƒ Ä‘á»™i ngÅ© ká»¹ thuáº­t giÃ¡m sÃ¡t vÃ  debug (Audit Trail).8  
* **VÃ²ng láº·p thá»±c thi (Execution Loop):** Há»‡ thá»‘ng backend cáº§n láº¯ng nghe token káº¿t thÃºc suy luáº­n. Khi gáº·p tháº» \<tool\_call\>, há»‡ thá»‘ng táº¡m dá»«ng sinh vÄƒn báº£n, thá»±c thi API thá»±c táº¿, vÃ  Ä‘Æ°a káº¿t quáº£ (JSON output) ngÆ°á»£c trá»Ÿ láº¡i vÃ o ngá»¯ cáº£nh chat Ä‘á»ƒ mÃ´ hÃ¬nh tiáº¿p tá»¥c sinh ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng cho ngÆ°á»i dÃ¹ng.

### **5.2 Káº¿t Luáº­n**

Viá»‡c xÃ¢y dá»±ng Chatbot L1 vá»›i kháº£ nÄƒng tÆ° duy nhÆ° nhÃ¢n viÃªn thá»±c thá»¥ khÃ´ng cÃ²n lÃ  viá»…n cáº£nh xa vá»i nhá» sá»± káº¿t há»£p giá»¯a phÆ°Æ¡ng phÃ¡p Agentic Fine-tuning vÃ  sá»©c máº¡nh tá»‘i Æ°u hÃ³a cá»§a Unsloth. Pipeline Ä‘Æ°á»£c Ä‘á» xuáº¥t trong bÃ¡o cÃ¡o nÃ yâ€”tá»« viá»‡c xÃ¢y dá»±ng Data Factory Ä‘áº¿n quy trÃ¬nh huáº¥n luyá»‡n kÃ©p SFT+GRPOâ€”cung cáº¥p má»™t lá»™ trÃ¬nh rÃµ rÃ ng, kháº£ thi vá» máº·t ká»¹ thuáº­t vÃ  hiá»‡u quáº£ vá» máº·t chi phÃ­ cho cÃ¡c doanh nghiá»‡p muá»‘n sá»Ÿ há»¯u há»‡ thá»‘ng AI tá»± chá»§ cao cáº¥p.

Báº±ng cÃ¡ch nhÃºng sÃ¢u quy trÃ¬nh (SOP) vÃ  vÄƒn phong (Persona) vÃ o trá»ng sá»‘ mÃ´ hÃ¬nh, doanh nghiá»‡p khÃ´ng chá»‰ giáº£m thiá»ƒu rá»§i ro áº£o giÃ¡c mÃ  cÃ²n táº¡o ra tráº£i nghiá»‡m khÃ¡ch hÃ ng Ä‘á»“ng nháº¥t, chuyÃªn nghiá»‡p, Ä‘Ã¡nh dáº¥u sá»± trÆ°á»Ÿng thÃ nh thá»±c sá»± cá»§a AI trong mÃ´i trÆ°á»ng doanh nghiá»‡p vÃ o nÄƒm 2025\.

### **TÃ i liá»‡u tham kháº£o & Nguá»“n dá»¯ liá»‡u**

* **Xu hÆ°á»›ng & Chiáº¿n lÆ°á»£c:** 1  
* **Ká»¹ thuáº­t Unsloth & Tá»‘i Æ°u hÃ³a:** 10  
* **Thuáº­t toÃ¡n GRPO & RL:** 12  
* **XÃ¢y dá»±ng Dataset & Dá»¯ liá»‡u tá»•ng há»£p:** 14  
* **VÄƒn phong & Persona:** 9  
* **Sá»­ dá»¥ng cÃ´ng cá»¥ & Suy luáº­n:** 8

#### **Works cited**

1. Top 10 trends in AI adoption for enterprises in 2025 \- Glean, accessed December 29, 2025, [https://www.glean.com/perspectives/enterprise-insights-from-ai](https://www.glean.com/perspectives/enterprise-insights-from-ai)  
2. What's next for AI? \- Deloitte, accessed December 29, 2025, [https://www.deloitte.com/us/en/insights/topics/technology-management/tech-trends/2025/tech-trends-ai-agents-and-autonomous-ai.html](https://www.deloitte.com/us/en/insights/topics/technology-management/tech-trends/2025/tech-trends-ai-agents-and-autonomous-ai.html)  
3. unslothai/unsloth: Fine-tuning & Reinforcement Learning for LLMs. ğŸ¦¥ Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM. \- GitHub, accessed December 29, 2025, [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)  
4. RAG vs Fine-Tuning 2026 What You Need to Know Before Implementation \- Kanerika, accessed December 29, 2025, [https://kanerika.com/blogs/rag-vs-fine-tuning/](https://kanerika.com/blogs/rag-vs-fine-tuning/)  
5. RAG vs fine tuning for help centers: The 2025 guide \- eesel AI, accessed December 29, 2025, [https://www.eesel.ai/blog/rag-vs-fine-tuning-for-help-centers](https://www.eesel.ai/blog/rag-vs-fine-tuning-for-help-centers)  
6. RAG vs. Fine-tuning \- IBM, accessed December 29, 2025, [https://www.ibm.com/think/topics/rag-vs-fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning)  
7. Advanced Techniques in Agent Fine-Tuning for 2025 \- Sparkco, accessed December 29, 2025, [https://sparkco.ai/blog/advanced-techniques-in-agent-fine-tuning-for-2025](https://sparkco.ai/blog/advanced-techniques-in-agent-fine-tuning-for-2025)  
8. DeepSeek R1 Quickstart \- Together.ai Docs, accessed December 29, 2025, [https://docs.together.ai/docs/deepseek-r1](https://docs.together.ai/docs/deepseek-r1)  
9. LLM Fineâ€‘Tuning in 2025: A Handsâ€‘On, Testâ€‘Driven Blueprint | by Carlos Esteban | Medium, accessed December 29, 2025, [https://medium.com/@tabers77/llm-fine-tuning-in-2025-a-hands-on-test-driven-blueprint-dd1c7887bb99](https://medium.com/@tabers77/llm-fine-tuning-in-2025-a-hands-on-test-driven-blueprint-dd1c7887bb99)  
10. Unsloth AI: A Deep Dive into Faster, More Efficient LLM Fine-Tuning \- Skywork.ai, accessed December 29, 2025, [https://skywork.ai/skypage/en/Unsloth-AI:-A-Deep-Dive-into-Faster,-More-Efficient-LLM-Fine-Tuning/1972856091659923456](https://skywork.ai/skypage/en/Unsloth-AI:-A-Deep-Dive-into-Faster,-More-Efficient-LLM-Fine-Tuning/1972856091659923456)  
11. Unsloth: Making LLM Fine-Tuning Fast, Cheap, and Practical | by Asimsultan (Head of AI) | Nov, 2025 | Medium, accessed December 29, 2025, [https://medium.com/@asimsultan2/unsloth-making-llm-fine-tuning-fast-cheap-and-practical-f324bcc98bd8](https://medium.com/@asimsultan2/unsloth-making-llm-fine-tuning-fast-cheap-and-practical-f324bcc98bd8)  
12. GRPO Fine-Tuning on DeepSeek-7B with Unsloth \- Analytics Vidhya, accessed December 29, 2025, [https://www.analyticsvidhya.com/blog/2025/02/grpo-fine-tuning-on-deepseek-7b/](https://www.analyticsvidhya.com/blog/2025/02/grpo-fine-tuning-on-deepseek-7b/)  
13. Fine-tuning Llama 3.2 and Using It Locally: A Step-by-Step Guide | DataCamp, accessed December 29, 2025, [https://www.datacamp.com/tutorial/fine-tuning-llama-3-2](https://www.datacamp.com/tutorial/fine-tuning-llama-3-2)  
14. Fine-Tuning LLMs for Efficient Agentic Tasks with Hyperstack AI Studio, accessed December 29, 2025, [https://www.hyperstack.cloud/technical-resources/tutorials/fine-tuning-llms-for-agentic-use-with-hyperstack-ai-studio](https://www.hyperstack.cloud/technical-resources/tutorials/fine-tuning-llms-for-agentic-use-with-hyperstack-ai-studio)  
15. chat\_template.jinja Â· unsloth/Llama-4-Scout-17B-16E-Instruct at main \- Hugging Face, accessed December 29, 2025, [https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct/blob/main/chat\_template.jinja](https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct/blob/main/chat_template.jinja)  
16. SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents \- arXiv, accessed December 29, 2025, [https://arxiv.org/html/2506.08119v1](https://arxiv.org/html/2506.08119v1)  
17. Fine-tuning LLMs for function-calling \- Wandb, accessed December 29, 2025, [https://wandb.ai/wandb/function-calling-finetuning/reports/Fine-tuning-LLMs-for-function-calling--VmlldzoxMjgxMTgxMg](https://wandb.ai/wandb/function-calling-finetuning/reports/Fine-tuning-LLMs-for-function-calling--VmlldzoxMjgxMTgxMg)  
18. Converting and Storing Text Chunks in JSONL Format | CodeSignal Learn, accessed December 29, 2025, [https://codesignal.com/learn/courses/chunking-and-storing-text-for-efficient-llm-processing/lessons/converting-and-storing-text-chunks-in-jsonl-format](https://codesignal.com/learn/courses/chunking-and-storing-text-for-efficient-llm-processing/lessons/converting-and-storing-text-chunks-in-jsonl-format)  
19. From PDFs to AI-ready structured data: a deep dive \- Explosion AI, accessed December 29, 2025, [https://explosion.ai/blog/pdfs-nlp-structured-data](https://explosion.ai/blog/pdfs-nlp-structured-data)  
20. Using LLMs for Synthetic Data Generation: The Definitive Guide \- Confident AI, accessed December 29, 2025, [https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms)  
21. Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use \- arXiv, accessed December 29, 2025, [https://arxiv.org/html/2504.04736v1](https://arxiv.org/html/2504.04736v1)  
22. GRPO \- Reward functions for medical reasoning : r/unsloth \- Reddit, accessed December 29, 2025, [https://www.reddit.com/r/unsloth/comments/1iw7675/grpo\_reward\_functions\_for\_medical\_reasoning/](https://www.reddit.com/r/unsloth/comments/1iw7675/grpo_reward_functions_for_medical_reasoning/)  
23. Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning, accessed December 29, 2025, [https://arxiv.org/html/2512.05747v1](https://arxiv.org/html/2512.05747v1)  
24. Fine-Tuning Made Fast : How Unsloth is Redefining the LLM Training Workflow \- Medium, accessed December 29, 2025, [https://medium.com/@mehtameet115/fine-tuning-made-fast-how-unsloth-is-redefining-the-llm-training-workflow-db511353957c](https://medium.com/@mehtameet115/fine-tuning-made-fast-how-unsloth-is-redefining-the-llm-training-workflow-db511353957c)  
25. GRPO Trainer \- Hugging Face, accessed December 29, 2025, [https://huggingface.co/docs/trl/main/en/grpo\_trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer)  
26. Fine-Tuning 1B LLaMA 3.2: A Comprehensive Step-by-Step Guide with Code, accessed December 29, 2025, [https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article](https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article)  
27. Finetuning gpt-oss-20b on custom tool calling. : r/unsloth \- Reddit, accessed December 29, 2025, [https://www.reddit.com/r/unsloth/comments/1oz9spx/finetuning\_gptoss20b\_on\_custom\_tool\_calling/](https://www.reddit.com/r/unsloth/comments/1oz9spx/finetuning_gptoss20b_on_custom_tool_calling/)  
28. 4\. Choosing the learning paradigm â€” From Text to Insight, accessed December 29, 2025, [https://matextract.pub/content/finetune/choosing\_paradigm.html](https://matextract.pub/content/finetune/choosing_paradigm.html)  
29. Deep dive into Group Relative Policy Optimization (GRPO) \- AWS Builder Center, accessed December 29, 2025, [https://builder.aws.com/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo](https://builder.aws.com/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo)  
30. Fine-Tuning LLMs: A Look at Group Relative Policy Optimization (GRPO) \- Medium, accessed December 29, 2025, [https://medium.com/@g.anirudh15/fine-tuning-llms-a-look-at-group-relative-policy-optimization-grpo-8240cac48ebc](https://medium.com/@g.anirudh15/fine-tuning-llms-a-look-at-group-relative-policy-optimization-grpo-8240cac48ebc)  
31. What should I expect from GPRO / adding reasoning to base model? : r/unsloth \- Reddit, accessed December 29, 2025, [https://www.reddit.com/r/unsloth/comments/1jcnx0b/what\_should\_i\_expect\_from\_gpro\_adding\_reasoning/](https://www.reddit.com/r/unsloth/comments/1jcnx0b/what_should_i_expect_from_gpro_adding_reasoning/)  
32. Train an LLM on NVIDIA Blackwell with Unslothâ€”and Scale for Production, accessed December 29, 2025, [https://developer.nvidia.com/blog/train-an-llm-on-an-nvidia-blackwell-desktop-with-unsloth-and-scale-it/](https://developer.nvidia.com/blog/train-an-llm-on-an-nvidia-blackwell-desktop-with-unsloth-and-scale-it/)  
33. Generating Synthetic Datasets for LLM Evaluators & Agents \- Phoenix \- Arize AI, accessed December 29, 2025, [https://arize.com/docs/phoenix/cookbook/tracing/generating-synthetic-datasets-for-llm-evaluators-and-agents](https://arize.com/docs/phoenix/cookbook/tracing/generating-synthetic-datasets-for-llm-evaluators-and-agents)  
34. Llama 4 Overpromises but Underdelivers \- unwind ai, accessed December 29, 2025, [https://www.theunwindai.com/p/llama-4-overpromises-but-underdelivers](https://www.theunwindai.com/p/llama-4-overpromises-but-underdelivers)  
35. \[2510.08191\] Training-Free Group Relative Policy Optimization \- arXiv, accessed December 29, 2025, [https://arxiv.org/abs/2510.08191](https://arxiv.org/abs/2510.08191)  
36. Flow-of-Action: SOP Enhanced LLM-Based Multi-Agent System for Root Cause Analysis, accessed December 29, 2025, [https://arxiv.org/html/2502.08224v1](https://arxiv.org/html/2502.08224v1)  
37. GRPO and the Future of LLM Fine-tuning: Moving Beyond Human Imitation \- Medium, accessed December 29, 2025, [https://medium.com/@andrecnf/grpo-and-the-future-of-llm-fine-tuning-moving-beyond-human-imitation-335dc14c2df9](https://medium.com/@andrecnf/grpo-and-the-future-of-llm-fine-tuning-moving-beyond-human-imitation-335dc14c2df9)